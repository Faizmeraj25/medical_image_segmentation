# -*- coding: utf-8 -*-
"""Retina_Vessel_standard.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1asnsmnVCaX_lW5Kb8HYkJy7iIUfJoyo5
"""

from google.colab import drive
drive.mount('/content/drive')

# Class to read image dataset.
import os
import numpy as np
import cv2
import torch
from torch.utils.data import Dataset

class DriveDataset(Dataset):
    def __init__(self, images_path, masks_path):

        self.images_path = images_path
        self.masks_path = masks_path
        self.n_samples = len(images_path)

    def __getitem__(self, index):
        """ Reading image """
        image = cv2.imread(self.images_path[index], cv2.IMREAD_COLOR)
        image = image/255.0 ## (512, 512, 3)
        image = np.transpose(image, (2, 0, 1))  ## (3, 512, 512)
        image = image.astype(np.float32)
        image = torch.from_numpy(image)

        """ Reading mask """
        mask = cv2.imread(self.masks_path[index], cv2.IMREAD_GRAYSCALE)
        mask = mask/255.0   ## (512, 512)
        mask = np.expand_dims(mask, axis=0) ## (1, 512, 512)
        mask = mask.astype(np.float32)
        mask = torch.from_numpy(mask)
#         print(image.shape, mask.shape)

        return image, mask

    def __len__(self):
        return self.n_samples

# Utils
import os
import time
import random
import numpy as np
import cv2
import torch

""" Seeding the randomness. """
def seeding(seed):
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True

""" Create a directory. """
def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

""" Calculate the time taken """
def epoch_time(start_time, end_time):
    elapsed_time = end_time - start_time
    elapsed_mins = int(elapsed_time / 60)
    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))
    return elapsed_mins, elapsed_secs

# Create a Model
# Creating the model.
import torch
import torch.nn as nn
from torchsummary import summary


class conv_block(nn.Module):
    def __init__(self, in_c, out_c):
        super().__init__()
        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_c)

        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_c)
        self.relu = nn.ReLU()

    def forward(self, inputs):
        x = self.conv1(inputs)
        x = self.bn1(x)
        x = self.relu(x)

        x = self.conv2(x)
        x = self.bn2(x)
        x = self.relu(x)

        return x

class encoder_block(nn.Module):
    def __init__(self, in_c, out_c):
        super().__init__()

        self.conv = conv_block(in_c, out_c)
        self.pool = nn.MaxPool2d((2, 2))

    def forward(self, inputs):
        x = self.conv(inputs)
        p = self.pool(x)

        return x, p

class decoder_block(nn.Module):
    def __init__(self, in_c, out_c):
        super().__init__()

        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)
        self.conv = conv_block(out_c+out_c, out_c)

    def forward(self, inputs, skip):
        x = self.up(inputs)
        x = torch.cat([x, skip], axis=1)
        x = self.conv(x)
        return x

class build_unet(nn.Module):
    def __init__(self):
        super().__init__()

        """ Encoder """
        self.e1 = encoder_block(3, 64)
        self.e2 = encoder_block(64, 128)
        self.e3 = encoder_block(128, 256)
        self.e4 = encoder_block(256, 512)

        """ Bottleneck """
        self.b = conv_block(512, 1024)

        """ Decoder """
        self.d1 = decoder_block(1024, 512)
        self.d2 = decoder_block(512, 256)
        self.d3 = decoder_block(256, 128)
        self.d4 = decoder_block(128, 64)

        """ Classifier """
        self.outputs = nn.Conv2d(64, 1, kernel_size=1, padding=0)

    def forward(self, inputs):
        """ Encoder """
        s1, p1 = self.e1(inputs)
        s2, p2 = self.e2(p1)
        s3, p3 = self.e3(p2)
        s4, p4 = self.e4(p3)
#         print(s1.shape, s2.shape, s3.shape, s4.shape)

        """ Bottleneck """
        b = self.b(p4)
#         print(b.shape)

        """ Decoder """
        d1 = self.d1(b, s4)
        d2 = self.d2(d1, s3)
        d3 = self.d3(d2, s2)
        d4 = self.d4(d3, s1)
#         print(d1.shape, d2.shape, d3.shape, d4.shape)
        outputs = self.outputs(d4)

        return outputs

# if __name__ == "__main__":
#     x = torch.randn((2, 3, 512, 512))
#     f = build_unet()
#     y = f(x)
#     # summary(build_unet, (3, 512, 512))
#     print(y.shape)

import torch
import torch.nn as nn
import torch.nn.functional as F

class DiceLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(DiceLoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):

        #comment out if your model contains a sigmoid or equivalent activation layer
        inputs = torch.sigmoid(inputs)

        #flatten label and prediction tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)

        intersection = (inputs * targets).sum()
        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)

        return 1 - dice

class DiceBCELoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(DiceBCELoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):

        #comment out if your model contains a sigmoid or equivalent activation layer
        inputs = torch.sigmoid(inputs)

        #flatten label and prediction tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)

        intersection = (inputs * targets).sum()
        dice_loss = 1 - (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)
        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')
        Dice_BCE = BCE + dice_loss

        return Dice_BCE

"""# Training"""

# Train the model
import os
import time
import torch
from glob import glob
from torch.utils.data import DataLoader
import torch.nn as nn
# Hyperparameters

Height = 512
Width = 512
size = (Height, Width)
batch_size = 1
learning_rate = 1e-4
num_workers = 0
pin_memory = True
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)
losses = {"train" : [], "val" : [] }
def train(model, loader, optimizer, loss_fn, device, metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0]):
    epoch_loss = 0.0
    model.train()

    for batch_ind, (x, y) in enumerate(loader):
        x = x.to(device, dtype=torch.float32)
        y = y.to(device, dtype=torch.float32)
        # print(x.shape, y.shape)
        optimizer.zero_grad()
        y_pred = model(x)
        # print("YPRED", y_pred)
        # print("Y", y)
        loss = loss_fn(y_pred, y)
        # y = y.to('cpu')
        # y_pred = y_pred.to('cpu')
        # score = calculate_metrics(y, y_pred) s_score, score))
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    epoch_loss = epoch_loss/len(loader)
    accuracy = metrics_score[4]/len(loader)
    return epoch_loss, accuracy

def evaluate(model, loader, loss_fn, device, metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0]):
    epoch_loss = 0.0

    model.eval()
    with torch.no_grad():
        for x, y in loader:
            x = x.to(device, dtype=torch.float32)
            y = y.to(device, dtype=torch.float32)
            # print("x shape", x.shape)
            # print("y shape", y.shape)
            y_pred = model(x)
            # print("y pred",y_pred.shape)
            score = calculate_metrics(y, y_pred)
            metrics_score = list(map(add, metrics_score, score))
            # print(score)
            loss = loss_fn(y_pred, y)
            epoch_loss += loss.item()

        epoch_loss = epoch_loss/len(loader)
        accuracy =  metrics_score[4]/len(loader)
    return epoch_loss, accuracy

if __name__ == "__main__":
    create_dir("files")
    train_path = '/content/drive/MyDrive/Colab Notebooks/Retina_vessel/train/'

    train_x = sorted(glob(os.path.join(train_path, "image", "*")))
    train_y = sorted(glob(os.path.join(train_path,"mask" ,"*")))

    val_path = '/content/drive/MyDrive/Colab Notebooks/Retina_vessel/val'
    valid_x = sorted(glob(os.path.join(val_path,"image" ,"*")))
    valid_y = sorted(glob(os.path.join(val_path,"mask" ,"*")))


    data_str = f"Dataset Size:\nTrain: {len(train_x)} - Valid: {len(valid_x)}\n"
    print(data_str)

    """ Hyperparameters """
    H = 512
    W = 512
    size = (H, W)
    batch_size = 2
    num_epochs = 50
    lr = 1e-4
    checkpoint_path = "files/checkpoint.pth"

    """ Dataset and loader """
    train_dataset = DriveDataset(train_x, train_y)
    valid_dataset = DriveDataset(valid_x, valid_y)

    train_loader = DataLoader(
        dataset=train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2
    )

    valid_loader = DataLoader(
        dataset=valid_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2
    )

    # device = torch.device('cuda')

    print("device", device)
    model = build_unet()
    model = model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)
    loss_fn = DiceBCELoss()

    """ Training the model """
    best_valid_loss = float("inf")
    # metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0]
    # valid_loss, val_accur = evaluate(model, valid_loader, loss_fn, device, metrics_score)
    # print('valid accur before training: ', val_accur)
    # print("num_epochs: ", num_epochs)
    for epoch in range(num_epochs):
        start_time = time.time()

        metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0]
        train_loss, train_accur = train(model, train_loader, optimizer, loss_fn, device, metrics_score)

        valid_loss, val_accur = evaluate(model, valid_loader, loss_fn, device, metrics_score)

        """ Saving the model """
        if valid_loss < best_valid_loss:
            data_str = f"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}"
            print(data_str)

            best_valid_loss = valid_loss
            torch.save(model.state_dict(), checkpoint_path)

        end_time = time.time()
        epoch_mins, epoch_secs = epoch_time(start_time, end_time)

        data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\n'
        data_str += f'\tTrain Loss: {train_loss:.3f}\n'
        data_str += f'\t Val. Loss: {valid_loss:.3f}\n'
        # data_str += f'\t Train Accuracy: {train_accur: .3f}\n'
        data_str += f'\t Val. Accuracy: {val_accur: .3f}\n'
        losses["train"].append(train_loss)
        losses["val"].append(valid_loss)
        print(data_str)

import matplotlib.pyplot as plt
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
epoch_arr = []
for i in range(20):
  epoch_arr.append(i)
plt.plot( losses["train"] , label='Training Loss')
plt.xticks(np.arange(0, 50, step=2))
plt.xlabel('Epoch')
plt.ylabel('Train Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(losses["val"], label='Validation Loss')
plt.xticks(np.arange(0, 50, step=2))
plt.xlabel('Epoch')
plt.ylabel('Val Loss')
plt.legend()

plt.tight_layout()
plt.show()
print(losses['train'])
print(losses['val'])

"""# Saving 1 Training & Testing image for each epoch while Training"""

# Train the model
import os
import time
import torch
from glob import glob
from torch.utils.data import DataLoader
import torch.nn as nn
# Hyperparameters

Height = 64
Width = 64
size = (Height, Width)
batch_size = 1
learning_rate = 1e-4
num_workers = 0
pin_memory = True
device = "cuda" if torch.cuda.is_available() else "cpu"
print(device)
num_epochs = 10

def train(model, loader, optimizer, loss_fn, device):
    epoch_loss = 0.0
    model.train()

    for batch_ind, (x, y) in enumerate(loader):
        x = x.to(device, dtype=torch.float32)
        y = y.to(device)
        # print(x.shape, y.shape)
        optimizer.zero_grad()
        y_pred = model(x)
        # print("YPRED", y_pred)
        # print("Y", y)
        loss = loss_fn(y_pred, y)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    epoch_loss = epoch_loss/len(loader)
    return epoch_loss

def evaluate(model, loader, loss_fn, device):
    epoch_loss = 0.0

    model.eval()
    with torch.no_grad():
        for x, y in loader:
            x = x.to(device, dtype=torch.float32)
            y = y.to(device, dtype=torch.float32)

            y_pred = model(x)
            loss = loss_fn(y_pred, y)
            epoch_loss += loss.item()

        epoch_loss = epoch_loss/len(loader)
    return epoch_loss

if __name__ == "__main__":
    create_dir("files")
    train_path = '/content/drive/MyDrive/Colab Notebooks/Retina_vessel/train/'

    train_x = sorted(glob(os.path.join(train_path, "image", "*")))
    train_y = sorted(glob(os.path.join(train_path,"mask" ,"*")))

    val_path = '/content/drive/MyDrive/Colab Notebooks/Retina_vessel/val'
    valid_x = sorted(glob(os.path.join(val_path,"image" ,"*")))
    valid_y = sorted(glob(os.path.join(val_path,"mask" ,"*")))


    data_str = f"Dataset Size:\nTrain: {len(train_x)} - Valid: {len(valid_x)}\n"
    print(data_str)

    """ Hyperparameters """
    H = 512
    W = 512
    size = (H, W)
    batch_size = 2
    num_epochs = 50
    lr = 1e-4
    checkpoint_path = "files/checkpoint.pth"

    """ Dataset and loader """
    train_dataset = DriveDataset(train_x, train_y)
    valid_dataset = DriveDataset(valid_x, valid_y)

    train_loader = DataLoader(
        dataset=train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=2
    )

    valid_loader = DataLoader(
        dataset=valid_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=2
    )

    # device = torch.device('cuda')

    print("device", device)
    model = build_unet()
    model = model.to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=lr)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)
    loss_fn = DiceBCELoss()

    """ Training the model """
    best_valid_loss = float("inf")

    for epoch in range(num_epochs):
        start_time = time.time()

        train_loss = train(model, train_loader, optimizer, loss_fn, device)
        valid_loss = evaluate(model, valid_loader, loss_fn, device)

        """ Saving the model """
        if valid_loss < best_valid_loss:
            data_str = f"Valid loss improved from {best_valid_loss:2.4f} to {valid_loss:2.4f}. Saving checkpoint: {checkpoint_path}"
            print(data_str)

            best_valid_loss = valid_loss
            torch.save(model.state_dict(), checkpoint_path)

        end_time = time.time()
        epoch_mins, epoch_secs = epoch_time(start_time, end_time)

        data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\n'
        data_str += f'\tTrain Loss: {train_loss:.3f}\n'
        data_str += f'\t Val. Loss: {valid_loss:.3f}\n'
        print(data_str)
        ## Take a train image and test image.
        train_img = glob("/content/drive/MyDrive/Colab Notebooks/Retina_vessel/Iteration/train/image/*")
        train_mask = glob("/content/drive/MyDrive/Colab Notebooks/Retina_vessel/Iteration/train/mask/*")
        test_img = glob("/content/drive/MyDrive/Colab Notebooks/Retina_vessel/Iteration/test/image/*")
        test_mask = glob("/content/drive/MyDrive/Colab Notebooks/Retina_vessel/Iteration/test/mask/*")
        print(train_img)
        print(test_img)
        print(train_mask)
        print(test_mask)
        break

"""# Testing"""

# Test Dataset

import os, time
from operator import add
import numpy as np
from glob import glob
import cv2
from tqdm import tqdm
import imageio
import torch
from sklearn.metrics import accuracy_score, f1_score, jaccard_score, precision_score, recall_score

# from model import build_unet
# from utils import create_dir, seeding

def calculate_metrics(y_true, y_pred):
    """ Ground truth """
    y_true = y_true.cpu().numpy()
    y_true = y_true > 0.5
    y_true = y_true.astype(np.uint8)
    y_true = y_true.reshape(-1)

    """ Prediction """
    y_pred = y_pred.cpu().numpy()
    y_pred = y_pred > 0.5
    y_pred = y_pred.astype(np.uint8)
    y_pred = y_pred.reshape(-1)

    score_jaccard = jaccard_score(y_true, y_pred)
    score_f1 = f1_score(y_true, y_pred)
    score_recall = recall_score(y_true, y_pred)
    score_precision = precision_score(y_true, y_pred)
    score_acc = accuracy_score(y_true, y_pred)

    return [score_jaccard, score_f1, score_recall, score_precision, score_acc]

def mask_parse(mask):
    mask = np.expand_dims(mask, axis=-1)    ## (512, 512, 1)
    mask = np.concatenate([mask, mask, mask], axis=-1)  ## (512, 512, 3)
    return mask

if __name__ == "__main__":
    """ Seeding """
    seeding(42)

    """ Folders """
    create_dir("results")

    """ Load dataset """

    test_x = sorted(glob("/content/drive/MyDrive/Colab Notebooks/Retina_vessel/test/image/*"))
    test_y = sorted(glob("/content/drive/MyDrive/Colab Notebooks/Retina_vessel/test/mask/*"))

    """ Hyperparameters """
    H = 512
    W = 512
    size = (W, H)
    checkpoint_path = "/content/drive/MyDrive/Colab Notebooks/Retina_vessel/files/checkpoint.pth"

    """ Load the checkpoint """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    model = build_unet()
    model = model.to(device)
    model.load_state_dict(torch.load(checkpoint_path, map_location=device))
    model.eval()

    metrics_score = [0.0, 0.0, 0.0, 0.0, 0.0]
    time_taken = []

    for i, (x, y) in tqdm(enumerate(zip(test_x, test_y)), total=len(test_x)):
        """ Extract the name """
        name = x.split("/")[-1].split(".")[0]

        """ Reading image """
        image = cv2.imread(x, cv2.IMREAD_COLOR) ## (512, 512, 3)
        ## image = cv2.resize(image, size)
        x = np.transpose(image, (2, 0, 1))      ## (3, 512, 512)
        x = x/255.0
        x = np.expand_dims(x, axis=0)           ## (1, 3, 512, 512)
        x = x.astype(np.float32)
        x = torch.from_numpy(x)
        x = x.to(device)

        """ Reading mask """
        mask = cv2.imread(y, cv2.IMREAD_GRAYSCALE)  ## (512, 512)
        ## mask = cv2.resize(mask, size)
        y = np.expand_dims(mask, axis=0)            ## (1, 512, 512)
        y = y/255.0
        y = np.expand_dims(y, axis=0)               ## (1, 1, 512, 512)
        y = y.astype(np.float32)
        y = torch.from_numpy(y)
        y = y.to(device)

        with torch.no_grad():
            """ Prediction and Calculating FPS """
            start_time = time.time()
            pred_y = model(x)
            pred_y = torch.sigmoid(pred_y)
            total_time = time.time() - start_time
            time_taken.append(total_time)


            score = calculate_metrics(y, pred_y)
            metrics_score = list(map(add, metrics_score, score))
            pred_y = pred_y[0].cpu().numpy()        ## (1, 512, 512)
            pred_y = np.squeeze(pred_y, axis=0)     ## (512, 512)
            pred_y = pred_y > 0.5
            pred_y = np.array(pred_y, dtype=np.uint8)

        """ Saving masks """
        ori_mask = mask_parse(mask)
        pred_y = mask_parse(pred_y)
        line = np.ones((size[1], 10, 3)) * 128

        cat_images = np.concatenate(
            [image, line, ori_mask, line, pred_y * 255], axis=1
        )
        cv2.imwrite(f"results/{name}.png", cat_images)

    jaccard = metrics_score[0]/len(test_x)
    f1 = metrics_score[1]/len(test_x)
    recall = metrics_score[2]/len(test_x)
    precision = metrics_score[3]/len(test_x)
    acc = metrics_score[4]/len(test_x)
    print(f"Jaccard: {jaccard:1.4f} - F1: {f1:1.4f} - Recall: {recall:1.4f} - Precision: {precision:1.4f} - Acc: {acc:1.4f}")

    fps = 1/np.mean(time_taken)
    print("FPS: ", fps)